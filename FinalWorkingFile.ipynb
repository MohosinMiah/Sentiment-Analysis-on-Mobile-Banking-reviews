{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Mobile Banking reviews\n",
    "\n",
    "The given task is to learn sentiment from all the given reviews and predict rating from a new review. This objective is attained by following the steps listed below:\n",
    "\n",
    "### Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import future\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from bs4 import BeautifulSoup  \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                test  sentiment\n",
      "0                                               text  sentiment\n",
      "1  For a movie that gets no respect there sure ar...          0\n",
      "2  Bizarre horror movie filled with famous faces ...          0\n",
      "3  A solid, if unremarkable film. Matthau, as Ein...          0\n",
      "4  It's a strange feeling to sit alone in a theat...          0\n"
     ]
    }
   ],
   "source": [
    "col_names = [\"test\", \"sentiment\"]\n",
    "\n",
    "data = pd.read_csv('train.csv',names=col_names,error_bad_lines=False)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labeling\n",
    "\n",
    "- In this step we clean the data and label the data as <br>\n",
    "  > 0 for Poor <br>\n",
    "  > 1 for Neutral <br>\n",
    "  > 2 for Good\n",
    "- Read the data from __'Amazon_Unlocked_Mobile.csv'__ and add new column for labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                test  sentiment\n",
      "0                                               text  sentiment\n",
      "1  For a movie that gets no respect there sure ar...          0\n",
      "2  Bizarre horror movie filled with famous faces ...          0\n",
      "3  A solid, if unremarkable film. Matthau, as Ein...          0\n",
      "4  It's a strange feeling to sit alone in a theat...          0\n"
     ]
    }
   ],
   "source": [
    "def label_data():\n",
    "      col_names = [\"test\", \"sentiment\"]\n",
    "      data = pd.read_csv('train.csv',names=col_names,error_bad_lines=False)    \n",
    "#     labels = []\n",
    "#     for cell in rows['sentiment']:\n",
    "#         if cell == 1:\n",
    "#             labels.append('1')   #Good\n",
    "#         else:\n",
    "#             labels.append('0')   #Poor\n",
    "            \n",
    "#     rows['label'] = labels\n",
    "#     del rows['review']\n",
    "#     return rows\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Remove all the rows containing blank cells. The resultant data is stored as __'labelled_dataset.csv'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    col_names = [\"test\", \"sentiment\"]\n",
    "    data = pd.read_csv('train.csv',names=col_names,error_bad_lines=False)    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                test  sentiment\n",
      "0                                               text  sentiment\n",
      "1  For a movie that gets no respect there sure ar...          0\n",
      "2  Bizarre horror movie filled with famous faces ...          0\n",
      "3  A solid, if unremarkable film. Matthau, as Ein...          0\n",
      "4  It's a strange feeling to sit alone in a theat...          0\n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "The following text preprocessing are implemented to convert raw reviews to cleaned review, so that it will be easier for us to do feature extraction in the next step.\n",
    "\n",
    "- remove html tags using BeautifulSoup\n",
    "- remove non-character such as digits and symbols\n",
    "- convert to lower case\n",
    "- remove stop words such as \"the\" and \"and\" if needed\n",
    "- convert to root words by stemming if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case \n",
    "    \n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True: # stemming\n",
    "#         stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "    \n",
    "    return( \" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEvaluation(predictions, y_test_set):\n",
    "    #Print model evaluation to predicted result \n",
    "    \n",
    "#     print \"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test_set, predictions))\n",
    "    print (\"\\nAccuracy on validation set: % 5.2f \" %(accuracy_score(y_test_set, predictions)))\n",
    "    #print \"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test_set, predictions))\n",
    "    print (\"\\nClassification report : \\n\", metrics.classification_report(y_test_set, predictions))\n",
    "    print (\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test_set, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "The sentiment analysis of given text can be done in two ways. First, we need to find a word embedding to convert a text into a numerical representation. Second, we fit the numerical representations of text to machine learning algorithms or deep learning architectures.\n",
    "\n",
    "One common approach of word embedding is frequency based embedding such as Bag of Words (BoW) model. BoW model learns a vocubulary list from a given corpus and represents each document based on some counting methods of words. In this part, we will explore the model performance of using BoW with supervised learning algorithms. Here's the workflow in this part.\n",
    "\n",
    "- Step 1 : Preprocess raw reviews to cleaned reviews\n",
    "- Step 2 : Create BoW using CountVectorizer / Tfidfvectorizer in sklearn\n",
    "- Step 3 : Transform review text to numerical representations (feature vectors)\n",
    "- Step 4 : Fit feature vectors to supervised learning algorithm (eg. Naive Bayes, Logistic regression, etc.)\n",
    "- Step 5 : Improve the model performance by GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                test  sentiment\n",
      "0                                               text  sentiment\n",
      "1  For a movie that gets no respect there sure ar...          0\n",
      "2  Bizarre horror movie filled with famous faces ...          0\n",
      "3  A solid, if unremarkable film. Matthau, as Ein...          0\n",
      "4  It's a strange feeling to sit alone in a theat...          0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     data = label_data()\n",
    "#     data = clean_data(data)        ----------\n",
    "    #prints first 5 rows of the dataset\n",
    "    print(data.head())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Plot distribution of rating\n",
    "#     plt.figure(figsize=(12,8))\n",
    "#     # sns.countplot(data['Rating'])\n",
    "#     data['sentiment'].value_counts().sort_index().plot(kind='bar')\n",
    "#     plt.title('Distribution of Rating')\n",
    "#     plt.xlabel('sentiment')\n",
    "#     plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#     # Plot number of reviews for top 20 brands\n",
    "#     brands = data[\"test\"].value_counts()\n",
    "#     # brands.count()\n",
    "#     plt.figure(figsize=(12,8))\n",
    "#     brands[:40].plot(kind='bar')\n",
    "#     plt.title(\"Top 40 review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Plot number of reviews for top 50 products\n",
    "#     products = data[\"test\"].value_counts()\n",
    "#     plt.figure(figsize=(12,8))\n",
    "#     products[:50].plot(kind='bar')\n",
    "#     plt.title(\"Number of Reviews for Top 50 Products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Plot distribution of review length\n",
    "#     review_length = data[\"test\"].dropna().map(lambda x: len(x))\n",
    "#     plt.figure(figsize=(12,8))\n",
    "#     review_length.loc[review_length < 1500].hist()\n",
    "#     plt.title(\"Distribution of Review Length\")\n",
    "#     plt.xlabel('Review length (Number of character)')\n",
    "#     plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #split data into training and testing set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data['test'], data['sentiment'], test_size=0.1, random_state=0)\n",
    "    \n",
    "    \n",
    "    #If the label are words instead of numbers, they can be replaced programmatically using following two lines\n",
    "    #y_train = label_binarize(y_train, classes=[0, 1, 2])\n",
    "#     x_test_cleaned = pd.read_csv('test.csv',names=col_names,error_bad_lines=False)\n",
    "#     x_test = x_test_cleaned[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Preprocess text data in training set and validation set\n",
    "    x_train_cleaned = []\n",
    "    x_test_cleaned = []\n",
    "\n",
    "    for d in x_train:\n",
    "        x_train_cleaned.append(cleanText(d))\n",
    "\n",
    "    for d in x_test:\n",
    "        x_test_cleaned.append(cleanText(d))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer with Mulinomial Naive Bayes (Benchmark Model)\n",
    "\n",
    "Now we have cleaned reviews, the next step is to convert the reviews into numerical representations for machine learning algorithm.\n",
    "\n",
    "In sklearn library, we can use CountVectorizer which implements both tokenization and occurrence counting in a single class. The output is a sparse matrix representation of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 70383 \n",
      "\n",
      "Show some feature names : \n",
      " ['aa', 'afterworld', 'analyst', 'armistead', 'awwwwww', 'beaches', 'biochemical', 'boos', 'budding', 'capri', 'chao', 'clanging', 'companies', 'cooper', 'crucial', 'deathscythe', 'desperadoes', 'dislocated', 'dragonballz', 'eerieness', 'entente', 'exist', 'feels', 'flounce', 'frumpish', 'gertrude', 'grading', 'haggard', 'heiden', 'honkin', 'ignorance', 'infringement', 'ishmael', 'judgment', 'kisses', 'larky', 'limos', 'luske', 'marguerite', 'meet', 'misawa', 'motorist', 'neanderthals', 'nozzle', 'organics', 'pantomime', 'perms', 'pleadings', 'preform', 'pucking', 'rantings', 'reified', 'revitalize', 'rout', 'savalas', 'selina', 'shoes', 'sled', 'soundtract', 'starring', 'subjecting', 'swith', 'tenderfoot', 'titillates', 'tricked', 'unclean', 'untapped', 'victimized', 'watling', 'withdrawal', 'yuletide']\n"
     ]
    }
   ],
   "source": [
    "    # Fit and transform the training data to a document-term matrix using CountVectorizer\n",
    "    countVect = CountVectorizer() \n",
    "    x_train_countVect = countVect.fit_transform(x_train_cleaned)\n",
    "    print (\"Number of features : %d \\n\" %len(countVect.get_feature_names())) #6378 \n",
    "    print (\"Show some feature names : \\n\", countVect.get_feature_names()[::1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Train MultinomialNB classifier\n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(x_train_countVect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set:  0.85 \n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.84      1241\n",
      "           1       0.83      0.89      0.86      1260\n",
      "\n",
      "    accuracy                           0.85      2501\n",
      "   macro avg       0.85      0.85      0.85      2501\n",
      "weighted avg       0.85      0.85      0.85      2501\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[1007  234]\n",
      " [ 138 1122]]\n",
      "['1' '0' '1' ... '1' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "    # Evaluate the model on validaton set\n",
    "    predictions = mnb.predict(countVect.transform(x_test_cleaned))\n",
    "    modelEvaluation(predictions, y_test)\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            12500\n",
      "1            12500\n",
      "sentiment        1\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer with Logistic Regression\n",
    "\n",
    "Some words might frequently appear but have little meaningful information about the sentiment of a particular review. Instead of using occurance counting, we can use tf-idf transform to scale down the impact of frequently appeared words in a given corpus.\n",
    "\n",
    "In sklearn library, we can use TfidfVectorizer which implements both tokenization and tf-idf weighted counting in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 25937 \n",
      "\n",
      "Show some feature names : \n",
      " ['00', 'ambiguous', 'barbaric', 'breasts', 'cheerfulness', 'consideration', 'decked', 'doped', 'eschews', 'flips', 'gorier', 'his', 'insisting', 'krige', 'mahatma', 'moms', 'oddest', 'perspectives', 'prophecy', 'rendition', 'satirized', 'siren', 'stepped', 'teenage', 'twentieth', 'vulgar']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Fit and transform the training data to a document-term matrix using TfidfVectorizer \n",
    "    tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\n",
    "    x_train_tfidf = tfidf.fit_transform(x_train)\n",
    "    print (\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\n",
    "    print (\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features = 25937\n",
      "\n",
      "Top 10 features with smallest coefficients :\n",
      "['worst' 'bad' 'awful' 'waste' 'boring' 'poor' 'terrible' 'nothing'\n",
      " 'worse' 'no']\n",
      "\n",
      "Top 10 features with largest coefficients : \n",
      "['great' 'excellent' 'best' 'perfect' 'wonderful' 'amazing' 'well' 'loved'\n",
      " 'love' 'favorite']\n"
     ]
    }
   ],
   "source": [
    "    # Look at the top 10 features with smallest and the largest coefficients\n",
    "    feature_names = np.array(tfidf.get_feature_names())\n",
    "    sorted_coef_index = lr.coef_[0].argsort()\n",
    "    print (\"Total number of features = \" + str(len(sorted_coef_index)))\n",
    "    print ('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "    print ('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '0' '1' ... '1' '0' '0']\n",
      "\n",
      "Accuracy on validation set:  0.88 \n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88      1241\n",
      "           1       0.89      0.87      0.88      1260\n",
      "\n",
      "    accuracy                           0.88      2501\n",
      "   macro avg       0.88      0.88      0.88      2501\n",
      "weighted avg       0.88      0.88      0.88      2501\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[1108  133]\n",
      " [ 161 1099]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Evaluate on the validaton set\n",
    "    predictions = lr.predict(tfidf.transform(x_test_cleaned))\n",
    "    print(predictions)\n",
    "\n",
    "    modelEvaluation(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM and Trees\n",
    "\n",
    "system will use two algorithms SVM and Random Forest to perform analysis and to determine which suits better for recommendation.\n",
    "\n",
    "### Support Vector Machine\n",
    "\n",
    "Here we implement multi-svm for sentiment analysis. More information about it can be found on [this](http://scikit-learn.org/stable/modules/svm.html) link. <br>\n",
    "\n",
    "- Tip 1 : Here, different kernels can be tried out. For example, linear, nonlinear, precomputed, rbf etc.\n",
    "- Tip 2 : The parameter values given below can be tweaked to obtain different results.\n",
    "\n",
    "### Using LinearSVC\n",
    "\n",
    "Here you can tweak the api parameters of LinearSVC as per your choice. Refer to [this](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) link for making any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n"
     ]
    }
   ],
   "source": [
    "    #x_train_subset = tfidf.transform(x_train_cleaned[:100])\n",
    "    x_train_input = tfidf.transform(x_train_cleaned)\n",
    "    svr_lin = LinearSVC(multi_class='ovr',C=1.0,loss='squared_hinge', dual=False)\n",
    "    svr_lin.fit(x_train_input, y_train)\n",
    "    y_svr_lin_predicted = svr_lin.predict(tfidf.transform(x_test_cleaned))\n",
    "#     print(y_svr_lin_predicted)\n",
    "    sample = [\"Bikas is very Bad.But some time it works beeter\"]\n",
    "    sample = tfidf.transform(sample).toarray()\n",
    "    sentiment = svr_lin.predict(sample)\n",
    "    print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n"
     ]
    }
   ],
   "source": [
    "# Test Manually \n",
    "sample = [\"Bikas is very Bad.But some time it works beeter.It is not okey to use\"]\n",
    "sample = tfidf.transform(sample).toarray()\n",
    "sentiment = svr_lin.predict(sample)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set:  0.88 \n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.88      1241\n",
      "           1       0.89      0.88      0.88      1260\n",
      "\n",
      "    accuracy                           0.88      2501\n",
      "   macro avg       0.88      0.88      0.88      2501\n",
      "weighted avg       0.88      0.88      0.88      2501\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[1102  139]\n",
      " [ 150 1110]]\n"
     ]
    }
   ],
   "source": [
    "    modelEvaluation(y_svr_lin_predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Model Evaluation\n",
    "\n",
    "There are multiple functions for model evaluation in scikit learn. To know more about them, please follow the below mentioned links\n",
    "- [accuracy score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score)\n",
    "- [f_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)\n",
    "- [f1_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)\n",
    "- [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8844462215113954\n"
     ]
    }
   ],
   "source": [
    "    print (str(metrics.accuracy_score(y_test, y_svr_lin_predicted)))\n",
    "#     print \"Fscore of this SVM = \" + str(metrics.precision_recall_fscore_support(y_test, y_svr_lin_predicted, pos_label=2, average='weighted'))\n",
    "#     print \"F-1 score of this SVM = \" + str(metrics.f1_score(y_test, y_svr_lin_predicted, pos_label=2, average='weighted'))\n",
    "#     print \"confusion matrix = \" + str(metrics.confusion_matrix(y_test, y_svr_lin_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#     not run this code \n",
    "#     print \"Accuracy of this SVM = \" + str(metrics.accuracy_score(y_test, y_svr_lin_predicted))\n",
    "#     print \"Fscore of this SVM = \" + str(metrics.precision_recall_fscore_support(y_test, y_svr_lin_predicted, pos_label=2, average='weighted'))\n",
    "#     print \"F-1 score of this SVM = \" + str(metrics.f1_score(y_test, y_svr_lin_predicted, pos_label=2, average='weighted'))\n",
    "#     print \"confusion matrix = \" + str(metrics.confusion_matrix(y_test, y_svr_lin_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Refer to [this](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#id1) link for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '0' '1' ... '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "    rand = RandomForestClassifier()\n",
    "    rand.fit(x_train_input, y_train)\n",
    "    y_rand_predicted = rand.predict(tfidf.transform(x_test_cleaned))\n",
    "    print(y_rand_predicted)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on validation set:  0.74 \n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.76      1241\n",
      "           1       0.80      0.67      0.72      1260\n",
      "\n",
      "    accuracy                           0.74      2501\n",
      "   macro avg       0.75      0.75      0.74      2501\n",
      "weighted avg       0.75      0.74      0.74      2501\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[1025  216]\n",
      " [ 422  838]]\n"
     ]
    }
   ],
   "source": [
    "    modelEvaluation(y_rand_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"Result Of Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print (rand.score(tfidf.transform(x_test_cleaned), y_test))\n",
    "    print(\"Result Of Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        Not Run\n",
    "#     print (\"Accuracy of Random Forest = \" + str(rand.score(tfidf.transform(x_test_cleaned), y_test)))\n",
    "#     print (\"Fscore of this SVM = \" + str(metrics.precision_recall_fscore_support(y_test, y_predicted, pos_label=2, average='weighted')))\n",
    "#     print (\"F-1 score of this SVM = \" + str(metrics.f1_score(y_test, y_predicted, pos_label=2, average='weighted')))\n",
    "#     print (\"confusion matrix = \" + str(metrics.confusion_matrix(y_test, y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Refer [this](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier) link for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    decTree = DecisionTreeClassifier()\n",
    "    decTree.fit(x_train_input, y_train)\n",
    "    y_decTree_predicted = decTree.predict(tfidf.transform(x_test_cleaned))\n",
    "#     sample = [\"Dhaka wasa water is not useable.Very bad\"]\n",
    "#     sample = tfidf.transform(sample).toarray()\n",
    "#     sentiment = clf.predict(sample)\n",
    "#     print(sentiment)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modelEvaluation(y_decTree_predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print (decTree.score(tfidf.transform(x_test_cleaned), y_test))\n",
    "#     print \"Fscore of this SVM = \" + str(metrics.precision_recall_fscore_support(y_test, y_decTree_predicted, pos_label=2, average='weighted'))\n",
    "#     print \"F-1 score of this SVM = \" + str(metrics.f1_score(y_test, y_decTree_predicted, pos_label=2, average='weighted'))\n",
    "#     print \"confusion matrix = \" + str(metrics.confusion_matrix(y_test, y_decTree_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     Not Run\n",
    "#     print \"Accuracy of Decision Tree = \" + str(decTree.score(tfidf.transform(x_test_cleaned), y_test))\n",
    "#     print \"Fscore of this SVM = \" + str(metrics.precision_recall_fscore_support(y_test, y_decTree_predicted, pos_label=2, average='weighted'))\n",
    "#     print \"F-1 score of this SVM = \" + str(metrics.f1_score(y_test, y_decTree_predicted, pos_label=2, average='weighted'))\n",
    "#     print \"confusion matrix = \" + str(metrics.confusion_matrix(y_test, y_decTree_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"test\"]\n",
    "# inserting column with static value in data frame \n",
    "# sentiment_col = pd.Series([]) \n",
    "datas = pd.read_csv('data_store/yes.csv',names=col_names,error_bad_lines=False)\n",
    "# displaying data frame again - Output 2 \n",
    "datas = datas['test'].tolist()\n",
    "# print(datas.head())\n",
    "# print(type(datas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "b Sentiment :  ['1']\n",
      "n Sentiment :  ['1']\n",
      "n Sentiment :  ['1']\n",
      "n Sentiment :  ['1']\n",
      "n Sentiment :  ['1']\n",
      "n Sentiment :  ['1']\n",
      "n Sentiment :  ['1']\n",
      "n Sentiment :  ['1']\n",
      "n Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "r Sentiment :  ['1']\n",
      "u Sentiment :  ['1']\n",
      "u Sentiment :  ['1']\n",
      "u Sentiment :  ['1']\n",
      "u Sentiment :  ['1']\n",
      "u Sentiment :  ['1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for data in datas:\n",
    "    sample = [data]\n",
    "    sample = tfidf.transform(sample)\n",
    "    sentiment = svr_lin.predict(sample)\n",
    "    print(data,\"Sentiment : \",sentiment)\n",
    "#     print('********************************',sentiment) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-9d57b22585ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(datas.shape)\n",
    "print(datas.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
